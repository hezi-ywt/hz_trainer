#!/bin/bash

# å†…å­˜ä¼˜åŒ–çš„DMDè®­ç»ƒè„šæœ¬

echo "ğŸš€ å¼€å§‹å†…å­˜ä¼˜åŒ–çš„DMDè®­ç»ƒ..."

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export CUDA_LAUNCH_BLOCKING=1
export TORCH_USE_CUDA_DSA=1

# æ¸…ç†GPUå†…å­˜
echo "ğŸ§¹ æ¸…ç†GPUå†…å­˜..."
python -c "
import torch
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    print(f'GPUå†…å­˜å·²æ¸…ç†ï¼Œå¯ç”¨å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')
"

# æ£€æŸ¥GPUçŠ¶æ€
echo "ğŸ“Š æ£€æŸ¥GPUçŠ¶æ€..."
nvidia-smi --query-gpu=index,name,memory.total,memory.used,memory.free --format=csv

# è®¾ç½®è¾ƒå°çš„world_sizeä»¥å‡å°‘å†…å­˜å ç”¨
export CUDA_VISIBLE_DEVICES=0
export WORLD_SIZE=2

echo "ğŸ”§ ä½¿ç”¨é…ç½®: WORLD_SIZE=$WORLD_SIZE, CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"

# è¿è¡Œè®­ç»ƒ
echo "ğŸ¯ å¯åŠ¨è®­ç»ƒ..."
python -m torch.distributed.launch \
    --nproc_per_node=2 \
    --nnodes=1 \
    --node_rank=0 \
    --master_addr=localhost \
    --master_port=29500 \
    train_dmd_test.py \
    --config configs/dmd_train_config.yaml \
    --seed 42

echo "âœ… è®­ç»ƒå®Œæˆ!"



